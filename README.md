# HTML-Hierarchical-Transformer-based-Multi-task-Learning-for-Volatility-Prediction

If you find this repository helps your research, please cite our following paper:

Yang, Linyi and Li, Jiazheng and Dong, Ruihai and Zhang, Yue and Smyth, Barry. NumHTML: Numeric-Oriented Hierarchical Transformer Model for Multi-task Financial Forecasting. Proceedings of the AAAI Conference on Artificial Intelligence 2022.

  @inproceedings{yang2022numhtml,
  title={NumHTML: Numeric-Oriented Hierarchical Transformer Model for Multi-task Financial Forecasting},
  author={Yang, Linyi and Li, Jiazheng and Dong, Ruihai and Zhang, Yue and Smyth, Barry},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={10},
  pages={11604--11612},
  year={2022}
  }

Linyi Yang, Tin Lok James Ng, Barry Smyth, Ruihai Dong. HTML: Hierarchical Transformer-based Multi-task Learning for Volatility Prediction. Proceedings of the The Web Conference 2020.

    @inproceedings{yang2020html,
    title={HTML: Hierarchical Transformer-based Multi-task Learning for Volatility Prediction},
    author={Yang, Linyi and Ng, Tin Lok James and Smyth, Barry and Dong, Ruihai},
    booktitle={Proceedings of The Web Conference 2020},
    pages={441--451},
    year={2020}
    }
    
## Dataset    
The token-level transformer relies on the pre-trained transformers, which can be downloed from [here](https://huggingface.co/).
<br>The raw dataset of the earnings call can be found from [[Qin and Yang, ACL-19]](https://github.com/GeminiLn/EarningsCall_Dataset).

## Model
We provide our code and data used for the paper. Our HTML model consists with token-level transformer and sentence-level transformer which can be found at the Model path. Also, we provide our experimental code using Multi-task settings and Single-task settings respectively.

## Contact
Any questions or queries feel free to email me at linyi.yang@insight-centre.org -- Thanks for reading.
